Sally He and David Kang
Prototype Write-up

1. Our program is implemented in python, although we are considering re-implementing it in C++ for the final version to improve performance. Broadly speaking, our program takes in a large training set of electricity inputs and prices, and tries to predict the prices on the testing data set. Our data is publicly available from misoenergy.com in the form of csv files, which we have extracted and re-organized into a training and testing set. Our data is processed by the program in the sense that the csv data is converted into a list format that the program recognizes.

Our program outputs a graph that displays the net profit/loss based on trades made from the program's price predictions, over the course of the testing period. Profit and losses are calculated as follows - when the neural net predicts the future price will be higher than the gov't forecast price, we buy the contract. When the neural net predicts a lower price, we short sell the contract. We do this for each hour and the graph reflects the return on this hypothetical portfolio strategy over time.


2. One realization we’ve made is that the data is extremely noisy. While this was to be expected, we may have underestimated the extent of the variation. The electricity data contains a plethora of inputs and it’s clear that no individual factor is a strong indicator of future prices. Since neural nets converge slowly even when the data is clean, it takes especially long when the data displays large amounts of variance. As a result, we have been forced to operate on a very small subset of the data (we are looking at just 4 of several hundred electricity nodes), and to limit the number of nodes (neurons) in our neural network. 

However, when we did so, we were able to more or less reliably generate a profit. This was actually extremely surprising. In the best case scenario, it suggests that there are systematic errors in the government's pricing model. However, this result is probably just an artifact of the specific subset of data we chose. We think that temporary weather conditions may have triggered a sudden spike in prices that isn't an exploitable pattern over the long run. Our immediate next steps are to confirm this result by testing the model over a variety of time periods.

We are optimistic that our results will only improve as we begin to parallelize the program. Doing so will allow us to add additional nodes to the neural net and to analyze the full dataset rather than a subset. Moreover, neural networks are sensitive to the learning algorithm that we pick. We currently use backpropagation with a linear learning rate. This has worked well on very simple data sets (such as the k means cluster data), but we are not convinced this is the best approach for larger, noisier data sets.  Parallelization will enable us to try a variety of different learning functions and find the optimal one.

3. In terms of reevaluating goals, we are now focused on improving performance in light of the slow computational speed of our prototype. We are considering re-implementing our neural net and parallelizing the matrix multiplication. We haven’t decided whether to use a map/reduce or GPU approach, but both should yield significant improvements to performance. We are also adjusting the project goals. Rather than attempt to treat each electricity node as independent to identify mis-pricings, we may consider examining the relationship between nodes and look for patterns that way. We have already started making the adjustment in the prototype. Instead of running the neural net for each energy node separately, we are combining the inputs for all the nodes into one training and testing dataset. 

4. The biggest challenge has been processing the data. Since there are thousands of csv files on the MISO website, it was not practical for us to download all the files manually. Instead, we were forced to create a python script that automated that process, which proved more challenging than initially thought. Our script basically simulates a user who clicks on each link. The script required extensive use of an external python library (splinter) and familiarity with html. When we were downloading weather data, we relied on a similar method to automate that process as well. Creating the neural network was a big challenge in its own right. Initially, it was not difficult to get an initial prototype working. However, it turns out that different kinds of tweaks need to be made to the neural net depending on the context. For instance, our neural net required the use of a “bias” node to improve the model’s flexibility with data outside the [0,1] range. In the process, we pored over dozens of papers, and we have found a model that should be compatible with our experiment.
